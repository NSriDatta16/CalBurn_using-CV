{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "c6a311a8-6724-4266-bd80-36bcb76d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a31c0e53-a25b-457e-8e28-6e9b8f29ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'C:\\Users\\srida\\Downloads\\Images_CNN\\train'\n",
    "test_dir = r'C:\\Users\\srida\\Downloads\\Images_CNN\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a053fac3-08e5-4bf2-b582-18bddb091f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_counts(directory):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_counts[class_name] = len(os.listdir(class_path))\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f9d4314-86e3-47a6-a84a-d3f5b872c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = get_class_counts(train_dir)\n",
    "sorted_classes = sorted(class_counts, key=class_counts.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fd02e28-749e-44e7-9928-d5dcc8a4f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_101_classes = sorted_classes[:101]\n",
    "least_36_classes = sorted_classes[101:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e503bc74-681b-4ac3-abce-ab0c8127ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1 = top_101_classes[:25]\n",
    "split_2 = top_101_classes[25:50]\n",
    "split_3 = top_101_classes[50:75]\n",
    "split_4 = top_101_classes[75:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98948b1f-341d-4217-b436-d1446e93bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a47208ea-d6cb-4485-a7ca-3731b380612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,\n",
    "#    horizontal_flip=True,fill_mode='nearest')\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,\n",
    "#    horizontal_flip=True,fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84a2c619-d3d2-4bed-b434-487e52509470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generators(train_dir, test_dir, classes, batch_size=32, target_size=(150, 150)):\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return train_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29901a-3b56-41d9-9410-3509f36c02dd",
   "metadata": {},
   "source": [
    "#### VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb3711d0-3dde-4a79-ab2f-25ae94774187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1976s\u001b[0m 3s/step - accuracy: 0.2020 - loss: 2.7905 - val_accuracy: 0.4032 - val_loss: 2.0575\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 990ms/step - accuracy: 0.4095 - loss: 1.9813 - val_accuracy: 0.4514 - val_loss: 1.8846\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 840ms/step - accuracy: 0.4880 - loss: 1.7168 - val_accuracy: 0.4410 - val_loss: 1.9192\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 835ms/step - accuracy: 0.5230 - loss: 1.5674 - val_accuracy: 0.4540 - val_loss: 1.8585\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 832ms/step - accuracy: 0.5640 - loss: 1.4138 - val_accuracy: 0.4458 - val_loss: 1.9110\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 832ms/step - accuracy: 0.6002 - loss: 1.2974 - val_accuracy: 0.4460 - val_loss: 1.9274\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 834ms/step - accuracy: 0.6346 - loss: 1.1650 - val_accuracy: 0.4564 - val_loss: 1.9495\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 834ms/step - accuracy: 0.6595 - loss: 1.0652 - val_accuracy: 0.4560 - val_loss: 2.0211\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 838ms/step - accuracy: 0.6943 - loss: 0.9772 - val_accuracy: 0.4458 - val_loss: 2.0635\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 848ms/step - accuracy: 0.7174 - loss: 0.8777 - val_accuracy: 0.4346 - val_loss: 2.2107\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 842ms/step - accuracy: 0.7359 - loss: 0.8186 - val_accuracy: 0.4420 - val_loss: 2.2144\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 878ms/step - accuracy: 0.7580 - loss: 0.7440 - val_accuracy: 0.4470 - val_loss: 2.2854\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 840ms/step - accuracy: 0.7735 - loss: 0.7020 - val_accuracy: 0.4424 - val_loss: 2.4082\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6205s\u001b[0m 10s/step - accuracy: 0.7888 - loss: 0.6386 - val_accuracy: 0.4358 - val_loss: 2.4341\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2042s\u001b[0m 3s/step - accuracy: 0.7959 - loss: 0.6003 - val_accuracy: 0.4156 - val_loss: 2.6364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18218d5e390>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_vgg16_model(num_classes, input_shape=(150, 150, 3)):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use the same training and validation code\n",
    "model_vgg16_1 = create_vgg16_model(num_classes=len(split_1))  # Example for split_1\n",
    "model_vgg16_1.fit(train_gen_1, epochs=15, validation_data=test_gen_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bbc119f-b36c-4bb4-95ad-d97943059da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 883ms/step - accuracy: 0.2877 - loss: 2.5037 - val_accuracy: 0.4798 - val_loss: 1.7934\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 887ms/step - accuracy: 0.5054 - loss: 1.6517 - val_accuracy: 0.5234 - val_loss: 1.6039\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 849ms/step - accuracy: 0.5762 - loss: 1.4004 - val_accuracy: 0.5184 - val_loss: 1.6185\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 832ms/step - accuracy: 0.6212 - loss: 1.2372 - val_accuracy: 0.5458 - val_loss: 1.5930\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 835ms/step - accuracy: 0.6652 - loss: 1.0841 - val_accuracy: 0.5388 - val_loss: 1.6275\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 836ms/step - accuracy: 0.6999 - loss: 0.9422 - val_accuracy: 0.5408 - val_loss: 1.6514\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 840ms/step - accuracy: 0.7348 - loss: 0.8345 - val_accuracy: 0.5252 - val_loss: 1.7689\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 833ms/step - accuracy: 0.7690 - loss: 0.7378 - val_accuracy: 0.5374 - val_loss: 1.7225\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 836ms/step - accuracy: 0.7888 - loss: 0.6609 - val_accuracy: 0.5260 - val_loss: 1.8775\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 831ms/step - accuracy: 0.8156 - loss: 0.5771 - val_accuracy: 0.5358 - val_loss: 1.8676\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 829ms/step - accuracy: 0.8248 - loss: 0.5399 - val_accuracy: 0.5340 - val_loss: 1.9932\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 828ms/step - accuracy: 0.8368 - loss: 0.4960 - val_accuracy: 0.5338 - val_loss: 2.0149\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 875ms/step - accuracy: 0.8531 - loss: 0.4414 - val_accuracy: 0.5240 - val_loss: 2.1245\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 1s/step - accuracy: 0.8720 - loss: 0.3838 - val_accuracy: 0.5188 - val_loss: 2.1896\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 944ms/step - accuracy: 0.8736 - loss: 0.3795 - val_accuracy: 0.5248 - val_loss: 2.3481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x181ffb2c320>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16_2 = create_vgg16_model(num_classes=len(split_2))\n",
    "model_vgg16_2.fit(train_gen_2, epochs=15, validation_data=test_gen_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed03e7ac-e6c6-4420-9d36-335eb789e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 903ms/step - accuracy: 0.2912 - loss: 2.4653 - val_accuracy: 0.4754 - val_loss: 1.8092\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 867ms/step - accuracy: 0.5114 - loss: 1.6428 - val_accuracy: 0.5202 - val_loss: 1.6575\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 860ms/step - accuracy: 0.5796 - loss: 1.3934 - val_accuracy: 0.5224 - val_loss: 1.6203\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 858ms/step - accuracy: 0.6387 - loss: 1.1923 - val_accuracy: 0.5320 - val_loss: 1.6383\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 856ms/step - accuracy: 0.6763 - loss: 1.0519 - val_accuracy: 0.5308 - val_loss: 1.6740\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 858ms/step - accuracy: 0.7161 - loss: 0.9216 - val_accuracy: 0.5390 - val_loss: 1.7328\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 861ms/step - accuracy: 0.7556 - loss: 0.7699 - val_accuracy: 0.5322 - val_loss: 1.7251\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 868ms/step - accuracy: 0.7772 - loss: 0.6933 - val_accuracy: 0.5418 - val_loss: 1.7835\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 855ms/step - accuracy: 0.8080 - loss: 0.5936 - val_accuracy: 0.5336 - val_loss: 1.8739\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 842ms/step - accuracy: 0.8181 - loss: 0.5586 - val_accuracy: 0.5196 - val_loss: 1.9936\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 840ms/step - accuracy: 0.8501 - loss: 0.4679 - val_accuracy: 0.5172 - val_loss: 2.0919\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 866ms/step - accuracy: 0.8540 - loss: 0.4445 - val_accuracy: 0.5258 - val_loss: 2.1330\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 970ms/step - accuracy: 0.8764 - loss: 0.3829 - val_accuracy: 0.5232 - val_loss: 2.2550\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 981ms/step - accuracy: 0.8779 - loss: 0.3628 - val_accuracy: 0.5296 - val_loss: 2.1962\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 967ms/step - accuracy: 0.8896 - loss: 0.3283 - val_accuracy: 0.5336 - val_loss: 2.2231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18226549d30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16_3 = create_vgg16_model(num_classes=len(split_3))\n",
    "model_vgg16_3.fit(train_gen_3, epochs=15, validation_data=test_gen_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43aa123f-d928-4598-8da5-ec940f1f9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 987ms/step - accuracy: 0.2356 - loss: 2.6640 - val_accuracy: 0.4285 - val_loss: 1.9190\n",
      "Epoch 2/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 956ms/step - accuracy: 0.4581 - loss: 1.8279 - val_accuracy: 0.4871 - val_loss: 1.7350\n",
      "Epoch 3/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 941ms/step - accuracy: 0.5174 - loss: 1.5811 - val_accuracy: 0.4967 - val_loss: 1.7120\n",
      "Epoch 4/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 902ms/step - accuracy: 0.5633 - loss: 1.4226 - val_accuracy: 0.5038 - val_loss: 1.6654\n",
      "Epoch 5/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 966ms/step - accuracy: 0.6011 - loss: 1.2862 - val_accuracy: 0.4973 - val_loss: 1.7391\n",
      "Epoch 6/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 974ms/step - accuracy: 0.6340 - loss: 1.1715 - val_accuracy: 0.5092 - val_loss: 1.7057\n",
      "Epoch 7/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 971ms/step - accuracy: 0.6687 - loss: 1.0638 - val_accuracy: 0.5092 - val_loss: 1.7393\n",
      "Epoch 8/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 959ms/step - accuracy: 0.6992 - loss: 0.9532 - val_accuracy: 0.5010 - val_loss: 1.7837\n",
      "Epoch 9/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 911ms/step - accuracy: 0.7192 - loss: 0.8797 - val_accuracy: 0.4910 - val_loss: 1.8393\n",
      "Epoch 10/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 933ms/step - accuracy: 0.7368 - loss: 0.8146 - val_accuracy: 0.4933 - val_loss: 1.9087\n",
      "Epoch 11/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 989ms/step - accuracy: 0.7575 - loss: 0.7440 - val_accuracy: 0.4875 - val_loss: 1.9687\n",
      "Epoch 12/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 997ms/step - accuracy: 0.7693 - loss: 0.7033 - val_accuracy: 0.4858 - val_loss: 2.0016\n",
      "Epoch 13/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 1s/step - accuracy: 0.7904 - loss: 0.6321 - val_accuracy: 0.4848 - val_loss: 2.0987\n",
      "Epoch 14/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 1s/step - accuracy: 0.8030 - loss: 0.5915 - val_accuracy: 0.4904 - val_loss: 2.1983\n",
      "Epoch 15/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 972ms/step - accuracy: 0.8219 - loss: 0.5294 - val_accuracy: 0.4852 - val_loss: 2.2834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18223fe5fd0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16_4 = create_vgg16_model(num_classes=len(split_4))\n",
    "model_vgg16_4.fit(train_gen_4, epochs=15, validation_data=test_gen_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "149d1fba-7fdb-4fc5-9f4b-ff795adf9977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m 3/78\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 2s/step - accuracy: 0.0278 - loss: 3.9584     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srida\\Anaconda\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.1761 - loss: 3.2621 - val_accuracy: 0.4710 - val_loss: 2.0176\n",
      "Epoch 2/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.5535 - loss: 1.5655 - val_accuracy: 0.5306 - val_loss: 1.6313\n",
      "Epoch 3/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 1s/step - accuracy: 0.7444 - loss: 0.9458 - val_accuracy: 0.5714 - val_loss: 1.4900\n",
      "Epoch 4/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - accuracy: 0.8597 - loss: 0.5758 - val_accuracy: 0.5903 - val_loss: 1.5025\n",
      "Epoch 5/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.8877 - loss: 0.4363 - val_accuracy: 0.5746 - val_loss: 1.5030\n",
      "Epoch 6/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.9404 - loss: 0.2731 - val_accuracy: 0.5746 - val_loss: 1.4898\n",
      "Epoch 7/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.9544 - loss: 0.2118 - val_accuracy: 0.5918 - val_loss: 1.4762\n",
      "Epoch 8/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.9791 - loss: 0.1356 - val_accuracy: 0.5918 - val_loss: 1.5213\n",
      "Epoch 9/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.9862 - loss: 0.1060 - val_accuracy: 0.5934 - val_loss: 1.5321\n",
      "Epoch 10/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.9808 - loss: 0.1032 - val_accuracy: 0.6044 - val_loss: 1.5087\n",
      "Epoch 11/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.9838 - loss: 0.0851 - val_accuracy: 0.5918 - val_loss: 1.5475\n",
      "Epoch 12/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2s/step - accuracy: 0.9891 - loss: 0.0736 - val_accuracy: 0.5808 - val_loss: 1.5878\n",
      "Epoch 13/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2s/step - accuracy: 0.9836 - loss: 0.0815 - val_accuracy: 0.6044 - val_loss: 1.6168\n",
      "Epoch 14/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2s/step - accuracy: 0.9921 - loss: 0.0547 - val_accuracy: 0.5997 - val_loss: 1.6502\n",
      "Epoch 15/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2s/step - accuracy: 0.9886 - loss: 0.0590 - val_accuracy: 0.6028 - val_loss: 1.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1822403dd00>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16_5 = create_vgg16_model(num_classes=len(least_36_classes))\n",
    "model_vgg16_5.fit(train_gen_5, epochs=15, validation_data=test_gen_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "40c7e22d-41f5-41c8-a76a-a2adabee129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models saved as .weights.h5 and .json files.\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models, 1):\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"model_vgg16_{i}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save_weights(f\"model_vgg16_{i}.weights.h5\")\n",
    "\n",
    "print(\"All models saved as .weights.h5 and .json files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7d2a012d-ef4a-4968-be4e-e561da8e276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models saved as complete .h5 files.\n"
     ]
    }
   ],
   "source": [
    "# Save each model as a complete .h5 file\n",
    "for i, model in enumerate(models, 1):\n",
    "    # Save the complete model (architecture + weights) in one file\n",
    "    model.save(f\"model_vgg16_{i}.h5\")\n",
    "\n",
    "print(\"All models saved as complete .h5 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c665132a-240a-4bc0-8e4a-2ce431d6f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    loaded_models = []\n",
    "    for i in range(1, 6):\n",
    "        # Load model architecture\n",
    "        with open(f\"model_vgg16_{i}.json\", \"r\") as json_file:\n",
    "            model_json = json_file.read()\n",
    "        model = model_from_json(model_json)\n",
    "        \n",
    "        # Load model weights\n",
    "        model.load_weights(f\"model_vgg16_{i}.h5\")\n",
    "        loaded_models.append(model)\n",
    "    \n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9a714e47-377c-444e-b92e-ce595dcb2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(image_input):\n",
    "    # Load the saved models\n",
    "    models = load_models()\n",
    "    \n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(image_input)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    confidence_scores = [np.max(pred) for pred in predictions]\n",
    "    best_model_idx = np.argmax(confidence_scores)\n",
    "    best_prediction = predictions[best_model_idx]\n",
    "    \n",
    "    predicted_class = np.argmax(best_prediction)\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "3d4b4b76-8b9a-446e-84af-3e9f1ca30707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model function saved as 'model.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ensemble_predict, f)\n",
    "\n",
    "print(\"Ensemble model function saved as 'model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95ccaa4a-aac2-40b7-9360-fe5132bc148b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    (\"saved_models/model_vgg16_1.json\", \"saved_models/model_vgg16_1.h5\"),\n",
    "    (\"saved_models/model_vgg16_2.json\", \"saved_models/model_vgg16_2.h5\"),\n",
    "    (\"saved_models/model_vgg16_3.json\", \"saved_models/model_vgg16_3.h5\"),\n",
    "    (\"saved_models/model_vgg16_4.json\", \"saved_models/model_vgg16_4.h5\"),\n",
    "    (\"saved_models/model_vgg16_5.json\", \"saved_models/model_vgg16_5.h5\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5e3fcf14-35da-4b2b-8a76-66fcfd6d4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(json_path, weights_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "    model = model_from_json(model_json)\n",
    "    model.load_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "123aff2c-8c92-495f-8048-95ff47bce599",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    (\"model_vgg16_1.json\", \"model_vgg16_1.h5\"),(\"model_vgg16_2.json\", \"model_vgg16_2.h5\"),(\"model_vgg16_3.json\", \"model_vgg16_3.h5\"),\n",
    "    (\"model_vgg16_4.json\", \"model_vgg16_4.h5\"),(\"model_vgg16_5.json\", \"model_vgg16_5.h5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5daa8b7e-5f94-48e6-a935-d56234708365",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [load_model(json_path, weights_path) for json_path, weights_path in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0a920536-08e3-4e3c-bf15-55bdcea708ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = models[0].input_shape[1:]\n",
    "ensemble_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9825df51-bb3a-48b5-92ab-54a8ceef6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = [model(ensemble_input) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f9f95357-fcab-441e-aeca-a78c065c8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_output(outputs):\n",
    "    stacked_outputs = tf.stack(outputs, axis=0)  # Shape: (num_models, batch_size, num_classes)\n",
    "    max_confidences = tf.reduce_max(stacked_outputs, axis=-1)  # Max confidence for each model\n",
    "    best_model_idx = tf.argmax(max_confidences, axis=0)  # Best model per sample\n",
    "    best_outputs = tf.gather(stacked_outputs, best_model_idx, axis=0, batch_dims=1)\n",
    "    return best_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b26b3ddb-f5ff-4e4c-8968-a36eaee39db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_output = Lambda(select_best_output, output_shape=(max([output.shape[-1] for output in model_outputs]),))(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7de7e83e-f997-4d4e-aef5-8212a5d09b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = Model(inputs=ensemble_input, outputs=ensemble_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "73950c1f-8658-403d-a2d7-51f835c4dbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined ensemble model saved as 'model.h5'\n"
     ]
    }
   ],
   "source": [
    "ensemble_model.save(\"model.h5\")\n",
    "print(\"Combined ensemble model saved as 'model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c34c6fd6-8526-487d-bc0e-8f194c591e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models' structures saved as model.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# List to store the JSON structures of each model\n",
    "models_json = []\n",
    "\n",
    "# Iterate through each model and save its structure to the list\n",
    "for i, model in enumerate(models, 1):\n",
    "    model_json = model.to_json()\n",
    "    models_json.append(model_json)\n",
    "\n",
    "# Save the combined list of models' JSON structures to a single file named model.json\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json.dump(models_json, json_file)\n",
    "\n",
    "print(\"All models' structures saved as model.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2e0dc-13ed-489a-829c-28b769ef12ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "19e16cce-cd98-4f76-a179-634d8c09f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used in model_vgg16_5:\n",
      "grapes\n",
      "peas\n",
      "pineapple\n",
      "turnip\n",
      "lettuce\n",
      "soy beans\n",
      "spinach\n",
      "cucumber\n",
      "onion\n",
      "cabbage\n",
      "garlic\n",
      "tomato\n",
      "bell pepper\n",
      "sweetcorn\n",
      "capsicum\n",
      "pear\n",
      "beetroot\n",
      "jalepeno\n",
      "kiwi\n",
      "chilli pepper\n",
      "corn\n",
      "mango\n",
      "eggplant\n",
      "watermelon\n",
      "paprika\n",
      "carrot\n",
      "lemon\n",
      "raddish\n",
      "cauliflower\n",
      "pomegranate\n",
      "potato\n",
      "banana\n",
      "orange\n",
      "sweetpotato\n",
      "apple\n",
      "ginger\n"
     ]
    }
   ],
   "source": [
    "class_names = list(train_gen_5.class_indices.keys())  # 'train_gen' is the training generator you used\n",
    "print(\"Classes used in model_vgg16_5:\")\n",
    "for class_name in class_names:\n",
    "    print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4343e0dd-3a46-4704-be73-76db5eec6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"apple_pie\", \"baby_back_ribs\", \"baklava\", \"beef_carpaccio\", \"beef_tartare\",\n",
    "    \"beet_salad\", \"beignets\", \"bibimbap\", \"bread_pudding\", \"breakfast_burrito\",\n",
    "    \"bruschetta\", \"caesar_salad\", \"cannoli\", \"caprese_salad\", \"carrot_cake\",\n",
    "    \"ceviche\", \"cheesecake\", \"cheese_plate\", \"chicken_curry\", \"chicken_quesadilla\",\n",
    "    \"chicken_wings\", \"chocolate_cake\", \"chocolate_mousse\", \"churros\", \"clam_chowder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "75ca85b1-e564-49c2-8330-5da5d1bd776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_size=(150, 150)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a24821bf-1134-44e7-b52b-5e823cb678ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_class(model, img_array):\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_class = class_names[predicted_class_idx]\n",
    "    return predicted_class, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "649c4d7a-494d-4b8f-b5d7-008f28b19d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image(img_path, predicted_class):\n",
    "    img = cv2.imread(img_path)\n",
    "    h, w, _ = img.shape\n",
    "    cv2.rectangle(img, (10, 10), (w-10, h-10), (0, 255, 0), 3)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, predicted_class, (10, 30), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "aaa8193f-ba84-482a-b115-edcdae52cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r\"C:\\Users\\srida\\Downloads\\Images_CNN\\train\\beet_salad\\75494.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "09847474-6fe4-4ccc-81bc-fd57abe81b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = preprocess_image(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "a225eb2d-b317-4fd7-a0fa-21362d094eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_class, prediction = predict_image_class(model_vgg16_3, img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2631848-9729-40da-af59-ca428fc46e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_prediction_on_image(img_path, predicted_class)\n",
    "\n",
    "\n",
    "\n",
    "models = [load_model(r\"C:\\Users\\srida\\model_vgg16_1.h5\"), \n",
    "          load_model(r\"C:\\Users\\srida\\model_vgg16_2.h5\"), \n",
    "          load_model(r\"C:\\Users\\srida\\model_vgg16_3.h5\"), \n",
    "          load_model(r\"C:\\Users\\srida\\model_vgg16_4.h5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "2d8b7d2d-d752-4c31-9fc8-2b0ae637d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the models to avoid the warning\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "e4e648b5-fbb4-48fe-b449-357e33b9a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[416], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m img_array \u001b[38;5;241m=\u001b[39m preprocess_image(img_path)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Get combined prediction\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m predicted_class_index \u001b[38;5;241m=\u001b[39m get_combined_prediction(img_array)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Print the predicted class label\u001b[39;00m\n\u001b[0;32m     79\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m class_labels[predicted_class_index]\n",
      "Cell \u001b[1;32mIn[416], line 64\u001b[0m, in \u001b[0;36mget_combined_prediction\u001b[1;34m(img_array)\u001b[0m\n\u001b[0;32m     61\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img_array)\n\u001b[0;32m     62\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mflatten())  \u001b[38;5;66;03m# Flatten to make sure the array is 1D\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)  \u001b[38;5;66;03m# Convert list of predictions into numpy array\u001b[39;00m\n\u001b[0;32m     65\u001b[0m avg_prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Average across models\u001b[39;00m\n\u001b[0;32m     66\u001b[0m predicted_class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(avg_prediction)  \u001b[38;5;66;03m# Get the class with highest probability\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# Define your model paths\n",
    "model_paths = [\n",
    "    r\"C:\\Users\\srida\\model_vgg16_1.h5\", \n",
    "    r\"C:\\Users\\srida\\model_vgg16_2.h5\", \n",
    "    r\"C:\\Users\\srida\\model_vgg16_3.h5\", \n",
    "    r\"C:\\Users\\srida\\model_vgg16_4.h5\"\n",
    "]\n",
    "\n",
    "# Load the models\n",
    "models = [load_model(path) for path in model_paths]\n",
    "\n",
    "# Combine class labels from all models\n",
    "class_labels = [\n",
    "    \"apple_pie\", \"baby_back_ribs\", \"baklava\", \"beef_carpaccio\", \"beef_tartare\", \n",
    "    \"beet_salad\", \"beignets\", \"bibimbap\", \"bread_pudding\", \"breakfast_burrito\", \n",
    "    \"bruschetta\", \"caesar_salad\", \"cannoli\", \"caprese_salad\", \"carrot_cake\", \n",
    "    \"ceviche\", \"cheesecake\", \"cheese_plate\", \"chicken_curry\", \"chicken_quesadilla\", \n",
    "    \"chicken_wings\", \"chocolate_cake\", \"chocolate_mousse\", \"churros\", \"clam_chowder\",\n",
    "    \n",
    "    \"club_sandwich\", \"crab_cakes\", \"creme_brulee\", \"croque_madame\", \"cup_cakes\", \n",
    "    \"deviled_eggs\", \"donuts\", \"dumplings\", \"edamame\", \"eggs_benedict\", \"escargots\", \n",
    "    \"falafel\", \"filet_mignon\", \"fish_and_chips\", \"foie_gras\", \"french_fries\", \n",
    "    \"french_onion_soup\", \"french_toast\", \"fried_calamari\", \"fried_rice\", \"frozen_yogurt\", \n",
    "    \"garlic_bread\", \"gnocchi\", \"greek_salad\", \"grilled_cheese_sandwich\",\n",
    "    \n",
    "    \"grilled_salmon\", \"guacamole\", \"gyoza\", \"hamburger\", \"hot_and_sour_soup\", \"hot_dog\", \n",
    "    \"huevos_rancheros\", \"hummus\", \"ice_cream\", \"lasagna\", \"lobster_bisque\", \n",
    "    \"lobster_roll_sandwich\", \"macaroni_and_cheese\", \"macarons\", \"miso_soup\", \"mussels\", \n",
    "    \"nachos\", \"omelette\", \"onion_rings\", \"oysters\", \"pad_thai\", \"paella\", \"pancakes\", \n",
    "    \"panna_cotta\", \"peking_duck\",\n",
    "\n",
    "    \"pho\", \"pizza\", \"pork_chop\", \"poutine\", \"prime_rib\", \"pulled_pork_sandwich\", \n",
    "    \"ramen\", \"ravioli\", \"red_velvet_cake\", \"risotto\", \"samosa\", \"sashimi\", \n",
    "    \"scallops\", \"seaweed_salad\", \"shrimp_and_grits\", \"spaghetti_bolognese\", \n",
    "    \"spaghetti_carbonara\", \"spring_rolls\", \"steak\", \"strawberry_shortcake\", \"sushi\", \n",
    "    \"tacos\", \"takoyaki\", \"tiramisu\", \"tuna_tartare\", \"waffles\"\n",
    "    \n",
    "    \"grapes\", \"peas\", \"pineapple\", \"turnip\", \"lettuce\", \"soy beans\", \"spinach\", \n",
    "    \"cucumber\", \"onion\", \"cabbage\", \"garlic\", \"tomato\", \"bell pepper\", \"sweetcorn\", \n",
    "    \"capsicum\", \"pear\", \"beetroot\", \"jalepeno\", \"kiwi\", \"chilli pepper\", \"corn\", \"mango\", \n",
    "    \"eggplant\", \"watermelon\", \"paprika\", \"carrot\", \"lemon\", \"raddish\", \"cauliflower\", \n",
    "    \"pomegranate\", \"potato\", \"banana\", \"orange\", \"sweetpotato\", \"apple\", \"ginger\"\n",
    "]\n",
    "\n",
    "def preprocess_image(img_path, target_size=(150, 150)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)  # Convert image to numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize image (if needed for your model)\n",
    "    return img_array\n",
    "\n",
    "# Get combined predictions from all models\n",
    "def get_combined_prediction(img_array):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(img_array)\n",
    "        predictions.append(pred.flatten())  # Flatten to make sure the array is 1D\n",
    "\n",
    "    predictions = np.array(predictions)  # Convert list of predictions into numpy array\n",
    "    avg_prediction = np.mean(predictions, axis=0)  # Average across models\n",
    "    predicted_class_index = np.argmax(avg_prediction)  # Get the class with highest probability\n",
    "    return predicted_class_index\n",
    "\n",
    "# Example: Path to image for prediction\n",
    "img_path = r\"C:\\Users\\srida\\Downloads\\Images_CNN\\train\\strawberry_shortcake\\94985.jpg\"\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = preprocess_image(img_path)\n",
    "\n",
    "# Get combined prediction\n",
    "predicted_class_index = get_combined_prediction(img_array)\n",
    "\n",
    "# Print the predicted class label\n",
    "predicted_class = class_labels[predicted_class_index]\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b06f84-5033-435f-89e1-29ea01388b43",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4af4a093-38d6-4bd1-8545-da2c1460b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(num_classes, input_shape=(150, 150, 3)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.05),  # Dropout after first Conv layer\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.05),  # Dropout after second Conv layer\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.05),  # Dropout after third Conv layer\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.05),  # Dropout before final output layer\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ea833acd-0ea7-4914-942d-3ab07ea0d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "def create_regularized_model(num_classes, input_shape=(150, 150, 3)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.05),  # Changed dropout to 0.05\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Callback for reducing learning rate when validation accuracy doesn't improve\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e2f39-1338-4024-977b-3136e81b6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43457972-6b36-4cc0-8cd0-0e5c725b1a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f67460-78fb-448d-bd67-ac222635f1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "910c11dd-6842-4527-972c-cfebecbcc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "target_size = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "275cc558-7750-4305-b052-f0c6c0a6be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 25 classes.\n",
      "Found 5000 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_1, test_gen_1 = create_generators(train_dir, test_dir, split_1, batch_size, target_size)\n",
    "model_1 = create_regularized_model(len(split_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "51be06a3-2e57-4a61-a3b7-a28dcb88c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels for train_gen_1:\n",
      "{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24}\n",
      "Class labels for test_gen_1:\n",
      "{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class labels for train_gen_1:\")\n",
    "print(train_gen_1.class_indices)\n",
    "\n",
    "print(\"Class labels for test_gen_1:\")\n",
    "print(test_gen_1.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1aebbe1a-c80b-46f5-acea-e7e2dd146a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image counts for each label in train_gen_1:\n",
      "apple_pie: 800\n",
      "baby_back_ribs: 800\n",
      "baklava: 800\n",
      "beef_carpaccio: 800\n",
      "beef_tartare: 800\n",
      "beet_salad: 800\n",
      "beignets: 800\n",
      "bibimbap: 800\n",
      "bread_pudding: 800\n",
      "breakfast_burrito: 800\n",
      "bruschetta: 800\n",
      "caesar_salad: 800\n",
      "cannoli: 800\n",
      "caprese_salad: 800\n",
      "carrot_cake: 800\n",
      "ceviche: 800\n",
      "cheesecake: 800\n",
      "cheese_plate: 800\n",
      "chicken_curry: 800\n",
      "chicken_quesadilla: 800\n",
      "chicken_wings: 800\n",
      "chocolate_cake: 800\n",
      "chocolate_mousse: 800\n",
      "churros: 800\n",
      "clam_chowder: 800\n",
      "Image counts for each label in test_gen_1:\n",
      "apple_pie: 200\n",
      "baby_back_ribs: 200\n",
      "baklava: 200\n",
      "beef_carpaccio: 200\n",
      "beef_tartare: 200\n",
      "beet_salad: 200\n",
      "beignets: 200\n",
      "bibimbap: 200\n",
      "bread_pudding: 200\n",
      "breakfast_burrito: 200\n",
      "bruschetta: 200\n",
      "caesar_salad: 200\n",
      "cannoli: 200\n",
      "caprese_salad: 200\n",
      "carrot_cake: 200\n",
      "ceviche: 200\n",
      "cheesecake: 200\n",
      "cheese_plate: 200\n",
      "chicken_curry: 200\n",
      "chicken_quesadilla: 200\n",
      "chicken_wings: 200\n",
      "chocolate_cake: 200\n",
      "chocolate_mousse: 200\n",
      "churros: 200\n",
      "clam_chowder: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def print_image_counts_per_label(generator, name=\"\"):\n",
    "    # Count occurrences of each class index in the generator\n",
    "    class_counts = dict(Counter(generator.classes))\n",
    "    \n",
    "    # Map counts back to class labels\n",
    "    label_counts = {label: class_counts[idx] for label, idx in generator.class_indices.items()}\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Image counts for each label in {name}:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "print_image_counts_per_label(train_gen_1, \"train_gen_1\")\n",
    "print_image_counts_per_label(test_gen_1, \"test_gen_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c2f73c3-162f-4b1d-bd85-c81cc959d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 25 classes.\n",
      "Found 5000 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_2, test_gen_2 = create_generators(train_dir, test_dir, split_2, batch_size, target_size)\n",
    "model_2 = create_regularized_model(len(split_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "42738486-dc1e-4a49-a233-d2bb11ab296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 25 classes.\n",
      "Found 5000 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_3, test_gen_3 = create_generators(train_dir, test_dir, split_3, batch_size, target_size)\n",
    "model_3 = create_regularized_model(len(split_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "67dd534c-1599-42f0-8ef8-d01c9ba1a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20800 images belonging to 26 classes.\n",
      "Found 5200 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_4, test_gen_4 = create_generators(train_dir, test_dir, split_4, batch_size, target_size)\n",
    "model_4 = create_regularized_model(len(split_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d0e345cb-b2d6-43f0-8dae-49ca0f0e9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2478 images belonging to 36 classes.\n",
      "Found 637 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_5, test_gen_5 = create_generators(train_dir, test_dir, least_36_classes, batch_size, target_size)\n",
    "model_5 = create_regularized_model(len(least_36_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "704688ab-c93a-4b7c-b862-49fb0f5d42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srida\\Anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 410ms/step - accuracy: 0.0900 - loss: 3.9100 - val_accuracy: 0.2030 - val_loss: 2.8936\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 199ms/step - accuracy: 0.2169 - loss: 2.8257 - val_accuracy: 0.2490 - val_loss: 2.7288\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 191ms/step - accuracy: 0.2498 - loss: 2.7110 - val_accuracy: 0.2496 - val_loss: 2.6844\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 185ms/step - accuracy: 0.2741 - loss: 2.6201 - val_accuracy: 0.2694 - val_loss: 2.6645\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 191ms/step - accuracy: 0.2971 - loss: 2.5566 - val_accuracy: 0.2848 - val_loss: 2.6232\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 237ms/step - accuracy: 0.3245 - loss: 2.4598 - val_accuracy: 0.2976 - val_loss: 2.6128\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 189ms/step - accuracy: 0.3510 - loss: 2.3907 - val_accuracy: 0.3076 - val_loss: 2.5834\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.3881 - loss: 2.3039 - val_accuracy: 0.3072 - val_loss: 2.6273\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 187ms/step - accuracy: 0.4159 - loss: 2.2398 - val_accuracy: 0.3104 - val_loss: 2.6399\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.4433 - loss: 2.1320 - val_accuracy: 0.3050 - val_loss: 2.6779\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.4736 - loss: 2.0544 - val_accuracy: 0.3156 - val_loss: 2.6701\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.4983 - loss: 1.9782 - val_accuracy: 0.3040 - val_loss: 2.7591\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.5467 - loss: 1.8599 - val_accuracy: 0.3118 - val_loss: 2.7909\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.5844 - loss: 1.7534 - val_accuracy: 0.3104 - val_loss: 2.8828\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 189ms/step - accuracy: 0.6239 - loss: 1.6447 - val_accuracy: 0.3022 - val_loss: 3.0237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x182586d48c0>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model_1.fit(train_gen_1, epochs=epochs, validation_data=test_gen_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a19d0-d101-42ac-8993-873b1ff04edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0fe9975a-c882-417e-9371-b72212f29f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 377ms/step - accuracy: 0.0841 - loss: 3.9343 - val_accuracy: 0.2128 - val_loss: 2.8621\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 187ms/step - accuracy: 0.2097 - loss: 2.8302 - val_accuracy: 0.2346 - val_loss: 2.7614\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.2494 - loss: 2.7154 - val_accuracy: 0.2716 - val_loss: 2.6914\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.2829 - loss: 2.6280 - val_accuracy: 0.2752 - val_loss: 2.6627\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.3152 - loss: 2.5449 - val_accuracy: 0.2866 - val_loss: 2.6274\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.3465 - loss: 2.4452 - val_accuracy: 0.2880 - val_loss: 2.6530\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.3741 - loss: 2.3700 - val_accuracy: 0.3196 - val_loss: 2.5988\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 185ms/step - accuracy: 0.4073 - loss: 2.2715 - val_accuracy: 0.3044 - val_loss: 2.6775\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.4382 - loss: 2.1792 - val_accuracy: 0.3236 - val_loss: 2.6179\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.4847 - loss: 2.0667 - val_accuracy: 0.3200 - val_loss: 2.6816\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.5143 - loss: 1.9800 - val_accuracy: 0.3282 - val_loss: 2.7122\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.5538 - loss: 1.8626 - val_accuracy: 0.3224 - val_loss: 2.7714\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.5828 - loss: 1.7764 - val_accuracy: 0.3292 - val_loss: 2.8229\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 187ms/step - accuracy: 0.6298 - loss: 1.6624 - val_accuracy: 0.3150 - val_loss: 2.9444\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.6718 - loss: 1.5406 - val_accuracy: 0.3194 - val_loss: 3.0177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18254ba4320>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model_2.fit(train_gen_2, epochs=epochs, validation_data=test_gen_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fd230026-6468-4cfc-a1d7-591a19d558b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 376ms/step - accuracy: 0.0846 - loss: 3.8797 - val_accuracy: 0.2066 - val_loss: 2.9005\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.2205 - loss: 2.8324 - val_accuracy: 0.2610 - val_loss: 2.7325\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.2758 - loss: 2.6760 - val_accuracy: 0.2872 - val_loss: 2.6680\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 198ms/step - accuracy: 0.3075 - loss: 2.5746 - val_accuracy: 0.3010 - val_loss: 2.6171\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 255ms/step - accuracy: 0.3368 - loss: 2.4930 - val_accuracy: 0.3108 - val_loss: 2.6056\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 345ms/step - accuracy: 0.3632 - loss: 2.4095 - val_accuracy: 0.3114 - val_loss: 2.6231\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 195ms/step - accuracy: 0.3914 - loss: 2.3288 - val_accuracy: 0.3180 - val_loss: 2.5657\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 199ms/step - accuracy: 0.4178 - loss: 2.2448 - val_accuracy: 0.3388 - val_loss: 2.5540\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 196ms/step - accuracy: 0.4450 - loss: 2.1742 - val_accuracy: 0.3386 - val_loss: 2.5637\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 197ms/step - accuracy: 0.4899 - loss: 2.0611 - val_accuracy: 0.3358 - val_loss: 2.5948\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 235ms/step - accuracy: 0.5120 - loss: 1.9895 - val_accuracy: 0.3436 - val_loss: 2.6296\n",
      "Epoch 12/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 194ms/step - accuracy: 0.5413 - loss: 1.9022 - val_accuracy: 0.3290 - val_loss: 2.7022\n",
      "Epoch 13/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.5730 - loss: 1.7966 - val_accuracy: 0.3424 - val_loss: 2.7581\n",
      "Epoch 14/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 187ms/step - accuracy: 0.6153 - loss: 1.6998 - val_accuracy: 0.3332 - val_loss: 2.8116\n",
      "Epoch 15/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 186ms/step - accuracy: 0.6492 - loss: 1.6038 - val_accuracy: 0.3378 - val_loss: 2.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18261712960>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model_3.fit(train_gen_3, epochs=epochs, validation_data=test_gen_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9ea13fc2-d397-498b-8dd4-b3825ec237b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 383ms/step - accuracy: 0.0830 - loss: 3.9494 - val_accuracy: 0.1798 - val_loss: 2.9605\n",
      "Epoch 2/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 188ms/step - accuracy: 0.2007 - loss: 2.8706 - val_accuracy: 0.2521 - val_loss: 2.7208\n",
      "Epoch 3/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 188ms/step - accuracy: 0.2597 - loss: 2.6949 - val_accuracy: 0.2731 - val_loss: 2.6616\n",
      "Epoch 4/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 188ms/step - accuracy: 0.2872 - loss: 2.5924 - val_accuracy: 0.2971 - val_loss: 2.5845\n",
      "Epoch 5/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 187ms/step - accuracy: 0.3273 - loss: 2.5001 - val_accuracy: 0.3115 - val_loss: 2.5616\n",
      "Epoch 6/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 188ms/step - accuracy: 0.3523 - loss: 2.4066 - val_accuracy: 0.3060 - val_loss: 2.5592\n",
      "Epoch 7/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 188ms/step - accuracy: 0.3817 - loss: 2.3149 - val_accuracy: 0.3173 - val_loss: 2.5690\n",
      "Epoch 8/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 188ms/step - accuracy: 0.4059 - loss: 2.2398 - val_accuracy: 0.3267 - val_loss: 2.5642\n",
      "Epoch 9/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 187ms/step - accuracy: 0.4360 - loss: 2.1713 - val_accuracy: 0.3200 - val_loss: 2.5882\n",
      "Epoch 10/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 189ms/step - accuracy: 0.4586 - loss: 2.0921 - val_accuracy: 0.3258 - val_loss: 2.6038\n",
      "Epoch 11/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 194ms/step - accuracy: 0.4891 - loss: 2.0050 - val_accuracy: 0.3171 - val_loss: 2.6904\n",
      "Epoch 12/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 189ms/step - accuracy: 0.5302 - loss: 1.9136 - val_accuracy: 0.3213 - val_loss: 2.6915\n",
      "Epoch 13/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 188ms/step - accuracy: 0.5529 - loss: 1.8211 - val_accuracy: 0.3269 - val_loss: 2.7417\n",
      "Epoch 14/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 188ms/step - accuracy: 0.5885 - loss: 1.7086 - val_accuracy: 0.3125 - val_loss: 2.8197\n",
      "Epoch 15/15\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 192ms/step - accuracy: 0.6250 - loss: 1.6423 - val_accuracy: 0.3073 - val_loss: 3.0227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x181fd8c0080>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model_4.fit(train_gen_4, epochs=epochs, validation_data=test_gen_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cafc7372-46d0-4e77-af78-4946fce0b5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srida\\Anaconda\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.0384 - loss: 5.5404 - val_accuracy: 0.0612 - val_loss: 4.2777\n",
      "Epoch 2/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 980ms/step - accuracy: 0.0855 - loss: 4.0615 - val_accuracy: 0.1240 - val_loss: 3.5675\n",
      "Epoch 3/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.1588 - loss: 3.4303 - val_accuracy: 0.2151 - val_loss: 3.1765\n",
      "Epoch 4/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 984ms/step - accuracy: 0.2211 - loss: 3.0510 - val_accuracy: 0.2653 - val_loss: 2.8534\n",
      "Epoch 5/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 994ms/step - accuracy: 0.3108 - loss: 2.7386 - val_accuracy: 0.2826 - val_loss: 2.7021\n",
      "Epoch 6/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 969ms/step - accuracy: 0.3668 - loss: 2.4682 - val_accuracy: 0.2826 - val_loss: 2.7223\n",
      "Epoch 7/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 975ms/step - accuracy: 0.3943 - loss: 2.3989 - val_accuracy: 0.3501 - val_loss: 2.5360\n",
      "Epoch 8/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 980ms/step - accuracy: 0.4712 - loss: 2.0904 - val_accuracy: 0.3595 - val_loss: 2.4938\n",
      "Epoch 9/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 985ms/step - accuracy: 0.4795 - loss: 2.0357 - val_accuracy: 0.3312 - val_loss: 2.5958\n",
      "Epoch 10/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 996ms/step - accuracy: 0.5410 - loss: 1.8976 - val_accuracy: 0.3595 - val_loss: 2.5133\n",
      "Epoch 11/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 997ms/step - accuracy: 0.5745 - loss: 1.7609 - val_accuracy: 0.3799 - val_loss: 2.4578\n",
      "Epoch 12/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 986ms/step - accuracy: 0.6411 - loss: 1.6011 - val_accuracy: 0.3925 - val_loss: 2.4674\n",
      "Epoch 13/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1000ms/step - accuracy: 0.6748 - loss: 1.4849 - val_accuracy: 0.3830 - val_loss: 2.5088\n",
      "Epoch 14/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 992ms/step - accuracy: 0.7029 - loss: 1.3637 - val_accuracy: 0.3956 - val_loss: 2.5050\n",
      "Epoch 15/15\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 980ms/step - accuracy: 0.7513 - loss: 1.2351 - val_accuracy: 0.3972 - val_loss: 2.5957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1818d683800>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model_5.fit(train_gen_5, epochs=epochs, validation_data=test_gen_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5877fbef-7b95-46bc-823d-324b3ec319a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selection ensemble saved as CNN_model1.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# List of preloaded models and their output classes\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "model_classes = [25, 25, 26, 25, 36]  # Adjust based on your actual model classes\n",
    "\n",
    "def predict_with_model_selection(image):\n",
    "    # Get predictions from each model\n",
    "    predictions = [model.predict(image) for model in models]\n",
    "    \n",
    "    # Find the model with the highest confidence score for the top prediction\n",
    "    max_confidences = [np.max(pred) for pred in predictions]\n",
    "    selected_model_idx = np.argmax(max_confidences)\n",
    "    \n",
    "    # Return prediction from the selected model\n",
    "    return predictions[selected_model_idx]\n",
    "\n",
    "# Example of using the function\n",
    "# Assuming `sample_image` is a preprocessed image for prediction\n",
    "# sample_image = ... # Your preprocessing code here\n",
    "# prediction = predict_with_model_selection(sample_image)\n",
    "\n",
    "# Save the prediction function as a pickle file for use\n",
    "with open(\"CNN_model1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(predict_with_model_selection, file)\n",
    "\n",
    "print(\"Model selection ensemble saved as CNN_model1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd579f4-8df8-4153-b67e-e6daa942f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\"model_1.h5\", \"model_2.h5\", \"model_3.h5\", \"model_4.h5\", \"model_5.h5\"]\n",
    "\n",
    "models_list = [models.load_model(path) for path in model_paths]\n",
    "\n",
    "class_splits = [split_1, split_2, split_3, split_4, least_36_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3220718-706b-4453-9c70-087ed8f208bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(150, 150)):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8a1fb-7cff-46f8-a5b7-d0f845fab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_models(image_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    model_confidences = []\n",
    "\n",
    "    for model, class_names in zip(models_list, class_splits):\n",
    "        predictions = model.predict(img_array)\n",
    "        max_confidence = np.max(predictions)\n",
    "        class_idx = np.argmax(predictions)\n",
    "        predicted_class = class_names[class_idx]\n",
    "        model_confidences.append((max_confidence, predicted_class))\n",
    "\n",
    "    best_confidence, best_prediction = max(model_confidences, key=lambda x: x[0])\n",
    "\n",
    "    print(f\"Best Model Prediction: {best_prediction} with confidence {best_confidence}\")\n",
    "    return best_prediction, best_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b001b21-ef8f-4635-adac-97f72e1aeb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'C:\\Users\\srida\\Downloads\\Images_CNN\\sample_image.jpg'\n",
    "predicted_class, confidence = predict_with_models(image_path)\n",
    "print(f\"The predicted class for the image is: {predicted_class} with confidence {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42697879-1f9d-42e2-a103-41104bb01f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\srida\\Downloads\\Images_CNN\\train\\strawberry_shortcake\\94985.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef337cc-6315-4d3d-b2da-1a3b2b4b44a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fff8f4-2c52-4031-a921-956b49667080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
